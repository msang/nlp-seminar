{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8881a96-801e-4e54-b68f-9706ccdfd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim matplotlib nltk numpy pandas scikit-learn seaborn spacy stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27447cda-20a4-4a63-a0bb-c48e1beb0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2303199-6329-420c-bc92-d120cc67d8e7",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19219b72-c712-4823-9086-47ccc38574b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#hs_dev = pd.read_csv(\"haspeede2_dev_taskAB.tsv\", sep=\"\\t\")\n",
    "hs_dev = pd.read_csv(\"haspeede2_dev_taskAB_anon_revised.tsv\")\n",
    "hs_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb98ef-a63d-492f-a48c-51bc985f0909",
   "metadata": {},
   "source": [
    "* Get label distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b57a4-98b7-474e-9c20-93e7524f7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "hs_raw = hs_dev.hs.value_counts()\n",
    "hs_norm = hs_dev.hs.value_counts(normalize=True)\n",
    "print(f'HS CLASS DISTRIBUTION \\n(raw) \\n{hs_raw} \\n(%) \\n{hs_norm}')\n",
    "ster_raw = hs_dev.stereotype.value_counts()\n",
    "ster_norm = hs_dev.stereotype.value_counts(normalize=True)\n",
    "print(f'\\nSTEREOTYPE CLASS DISTRIBUTION \\n(raw) \\n{ster_raw} \\n(%) \\n{ster_norm}')\n",
    "hs_ster = hs_dev.groupby('hs')['stereotype'].value_counts()\n",
    "print(f'\\nCO-OCCURRENCE STATISTICS: \\n{hs_ster} \\n')\n",
    "#print(sns.barplot(x=hs_norm.index, y=hs_norm))\n",
    "print(sns.barplot(x=ster_norm.index, y=ster_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b3e59e-37f6-4078-be17-88938fb0bdf1",
   "metadata": {},
   "source": [
    "* Get most frequent terms and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5721869-9de4-4e66-9b90-706864ad07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams, FreqDist\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "STOPWORDS = get_stop_words('it') + [\"url\", \"user\", \"@user\"]\n",
    "text = [item for tweet in hs_dev['full_text'] for item in tweet.split(\" \") if item.lower() not in STOPWORDS]\n",
    "hs_text = [item for tweet in hs_dev[hs_dev.hs == 1]['full_text'] for item in tweet.split(\" \") if item.lower() not in STOPWORDS]\n",
    "ster_text = [item for tweet in hs_dev[hs_dev.stereotype == 1]['full_text'] for item in tweet.split(\" \") if item.lower() not in STOPWORDS]\n",
    "\n",
    "#find most common terms\n",
    "fdist = FreqDist(ster_text)\n",
    "fdist.most_common(20)\n",
    "\n",
    "\"\"\"\n",
    "#find most common bigrams\n",
    "bgs = bigrams(ster_text)\n",
    "fdist = FreqDist(bgs)\n",
    "fdist.most_common(20)\n",
    "\n",
    "#find most common trigrams\n",
    "tgs = trigrams(text)\n",
    "fdist = FreqDist(tgs)\n",
    "fdist.most_common(20)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b432c-69c5-4c60-85c9-06a610f04cdc",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing\n",
    "\n",
    "* tokenization\n",
    "* stopwords+punctuation removal\n",
    "* lowercasing\n",
    "* lemmatization\n",
    "* ... and many other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380db35-ff9a-4b50-8259-8d962b7a355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, spacy, string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "def preprocess(sentence):\n",
    "    #remove digits\n",
    "    text = re.sub('\\d+', '', sentence)\n",
    "    #remove extra whitespaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    #tokenization\n",
    "    text = word_tokenize(text)\n",
    "    #stopwords/punctuation removal + lowercasing\n",
    "    text = [token.lower() for token in text if token.lower() not in STOPWORDS and token not in string.punctuation]\n",
    "    #lemmatization\n",
    "    #sent = \" \".join(text)\n",
    "    #nlp = spacy.load(\"it_core_news_sm\")\n",
    "    #text = [token.lemma_ for token in nlp(sent)]\n",
    "    #stemming\n",
    "    stemmer = SnowballStemmer('italian')\n",
    "    text = [stemmer.stem(token) for token in text]\n",
    "    #other possible operations: handle emojis/emoticons, hashtags, URLs/email addresses, twitter handles \n",
    "    return \" \".join(text) \n",
    "  \n",
    "\n",
    "hs_dev['preprocessed'] = hs_dev['full_text'].apply(lambda x: preprocess(x))\n",
    "hs_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1caa7-5ab1-42b9-b5f3-a8a4bac4a13a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train-Test/Validation split\n",
    "(In this case we already have a held-out test set, but you might want to use a validation set as well for hyper-parameter tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cb891-5037-4849-aa54-c357fa18c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#if you split into train and validation sets:\n",
    "X_train_hs, X_val_hs, y_train_hs, y_val_hs = train_test_split(hs_dev[\"preprocessed\"],hs_dev[\"hs\"],test_size=0.1, shuffle=False)\n",
    "\n",
    "#if you just use the whole training set without hyper-parameter tuning:\n",
    "X_train_hs = hs_dev[\"preprocessed\"]\n",
    "y_train_hs = hs_dev[\"hs\"]\n",
    "\n",
    "X_train_hs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f621a0-d7d2-4f5b-a544-0bf49fcbe693",
   "metadata": {},
   "source": [
    "# Feature extraction and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66791ad-6ace-4f3a-bfcd-f2496ecd37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#\"\"\"\n",
    "#create feature vectors\n",
    "count = CountVectorizer(analyzer='word', max_features=5000)\n",
    "count_train = count.fit_transform(X_train_hs)\n",
    "tfidf = TfidfVectorizer(analyzer='word', max_features=5000)\n",
    "tfidf_train = tfidf.fit_transform(X_train_hs)\n",
    "trg = TfidfVectorizer(analyzer='word', ngram_range= (1,3), max_features=5000)\n",
    "trg_train = trg.fit_transform(X_train_hs)\n",
    "\n",
    "#fit the classifier on the training data\n",
    "svm = LinearSVC() # as classifier, we just use a linear SVM with default parameters\n",
    "svm_tfidf = LinearSVC()\n",
    "svm_trg = LinearSVC()\n",
    "svm.fit(count_train, y_train_hs)\n",
    "svm_tfidf.fit(tfidf_train, y_train_hs)\n",
    "svm_trg.fit(trg_train, y_train_hs)\n",
    "#\"\"\"\n",
    "\n",
    "\"\"\" alternative code using make_pipeline:\n",
    "count = CountVectorizer(analyzer='word', max_features=5000)\n",
    "tfidf = TfidfVectorizer(analyzer='word', max_features=5000)\n",
    "tfidf_trg = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=5000)\n",
    "\n",
    "svm = LinearSVC()  \n",
    "svm_tfidf = LinearSVC()\n",
    "svm_trg = LinearSVC()\n",
    "\n",
    "pipe_count = make_pipeline(count, svm)\n",
    "pipe_tfidf = make_pipeline(tfidf, svm_tfidf)\n",
    "pipe_trigrams = make_pipeline(tfidf_trg, svm_trg)\n",
    "\n",
    "pipe_count.fit(X_train_hs, y_train_hs)\n",
    "pipe_tfidf.fit(X_train_hs, y_train_hs)\n",
    "pipe_trigrams.fit(X_train_hs, y_train_hs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443801b-8296-4c1f-a3ca-7d2a73ab35a3",
   "metadata": {},
   "source": [
    "# Evaluation and error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d955b2b6-0884-4f54-b505-825e9f019da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#hs_test = pd.read_csv(\"haspeede2_reference_taskAB-tweets.tsv\", sep=\"\\t\", names=['id','text','hs','stereotype'])\n",
    "hs_test = pd.read_csv(\"haspeede2_reference_taskAB-tweets_anon_revised.tsv\")\n",
    "#hs_test = pd.read_csv(\"haspeede2_reference_taskAB-news_anon_revised.tsv\")\n",
    "hs_test\n",
    "\n",
    "X_test_hs = hs_test['full_text'].apply(lambda x: preprocess(x))\n",
    "y_test_hs = hs_test['hs']\n",
    "\n",
    "#\"\"\"\n",
    "count_test = count.transform(X_test_hs)\n",
    "tfidf_test = tfidf.transform(X_test_hs)\n",
    "trg_test = trg.transform(X_test_hs)\n",
    "\n",
    "y_pred_count = svm.predict(count_test)\n",
    "y_pred_tfidf = svm_tfidf.predict(tfidf_test)\n",
    "y_pred_trg = svm_trg.predict(trg_test)\n",
    "#\"\"\"\n",
    "\"\"\"\n",
    "y_pred_count = pipe_count.predict(X_test_hs)\n",
    "y_pred_tfidf = pipe_tfidf.predict(X_test_hs)\n",
    "y_pred_trg = pipe_trigrams.predict(X_test_hs)\n",
    "\"\"\"\n",
    "\n",
    "print('COUNT VECTORS: \\n\\nCONFUSION MATRIX ')       \n",
    "print(confusion_matrix(y_test_hs, y_pred_count), '\\n') \n",
    "print('EVALUATION METRICS \\n',classification_report(y_test_hs, y_pred_count))\n",
    "print('TF-IDF VECTORS: \\n\\nCONFUSION MATRIX ')       \n",
    "print(confusion_matrix(y_test_hs, y_pred_tfidf), '\\n') \n",
    "print('EVALUATION METRICS \\n',classification_report(y_test_hs, y_pred_tfidf))\n",
    "print('TF-IDF VECTORS + TRIGRAMS: \\n\\nCONFUSION MATRIX ')  \n",
    "print(confusion_matrix(y_test_hs, y_pred_trg), '\\n') \n",
    "print('EVALUATION METRICS \\n',classification_report(y_test_hs, y_pred_trg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a67d8c-aebf-4dd8-9cd4-d7a4c24e523d",
   "metadata": {},
   "source": [
    "Try using visual libraries to explain system's predictions, like [eli5](https://github.com/TeamHG-Memex/eli5) or \n",
    "[LIME](https://marcotcr.github.io/lime/). Both packages provide nice tutorials. Here an example using eli5 (source available [here](https://github.com/TeamHG-Memex/eli5/blob/master/notebooks/Debugging%20scikit-learn%20text%20classification%20pipeline.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13950d0d-2c30-4891-858b-d9a1ad23705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "\n",
    "eli5.show_weights(svm_tfidf, vec=tfidf, top=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13970cdb-0794-420a-90e7-d61a3675b44b",
   "metadata": {},
   "source": [
    "Try using the best-performing model with some brand new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02207713-d85c-4b97-8774-2ca74a09c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [preprocess(\"...\")] #put here some made-up sentence, just to see how the model goes\n",
    "new = tfidf.transform(sent)\n",
    "y_pred_tfidf = svm_tfidf.predict(new)\n",
    "print(sent, y_pred_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d1f7b-92db-4d92-9163-10bea9d08fdf",
   "metadata": {},
   "source": [
    "To perform <b>error analysis</b>:\n",
    "* select a sample of mislabeled data\n",
    "* compare results with gold annotation\n",
    "* get insights on possible causes of misclassification (also defining patterns, if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f152d-260e-452a-bb91-55f05f2b53b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
