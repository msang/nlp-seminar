{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8881a96-801e-4e54-b68f-9706ccdfd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim matplotlib nltk numpy pandas scikit-learn seaborn spacy stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27447cda-20a4-4a63-a0bb-c48e1beb0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdb6476-34af-437e-9ba4-cae2a67fda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b84796bf-445f-4d38-88a7-8fa3f37fb8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"it_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2303199-6329-420c-bc92-d120cc67d8e7",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19219b72-c712-4823-9086-47ccc38574b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hs</th>\n",
       "      <th>stereotype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2066</td>\n",
       "      <td>È terrorismo anche questo, per mettere in uno ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2045</td>\n",
       "      <td>@user @user infatti finché ci hanno guadagnato...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>Corriere: Tangenti, Mafia Capitale dimenticata...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1259</td>\n",
       "      <td>@user ad uno ad uno, perché quando i migranti ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>949</td>\n",
       "      <td>Il divertimento del giorno? Trovare i patrioti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>9340</td>\n",
       "      <td>Gli stati nazionali devono essere pronti a rin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>9121</td>\n",
       "      <td>Il ministro dell'interno della Germania #Horst...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>8549</td>\n",
       "      <td>#Salvini: In Italia troppi si sono montati la ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6837</th>\n",
       "      <td>9240</td>\n",
       "      <td>@user @user Chi giubila in buona fede non ha c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>8000</td>\n",
       "      <td>I giovani cristiani in #Etiopia sono indotti d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6839 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                          full_text  hs  stereotype\n",
       "0     2066  È terrorismo anche questo, per mettere in uno ...   0           0\n",
       "1     2045  @user @user infatti finché ci hanno guadagnato...   0           0\n",
       "2       61  Corriere: Tangenti, Mafia Capitale dimenticata...   0           0\n",
       "3     1259  @user ad uno ad uno, perché quando i migranti ...   0           0\n",
       "4      949  Il divertimento del giorno? Trovare i patrioti...   0           0\n",
       "...    ...                                                ...  ..         ...\n",
       "6834  9340  Gli stati nazionali devono essere pronti a rin...   0           0\n",
       "6835  9121  Il ministro dell'interno della Germania #Horst...   0           0\n",
       "6836  8549  #Salvini: In Italia troppi si sono montati la ...   0           0\n",
       "6837  9240  @user @user Chi giubila in buona fede non ha c...   0           0\n",
       "6838  8000  I giovani cristiani in #Etiopia sono indotti d...   0           1\n",
       "\n",
       "[6839 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#hs_dev = pd.read_csv(\"haspeede2_dev_taskAB.tsv\", sep=\"\\t\")\n",
    "hs_dev = pd.read_csv(\"haspeede2_dev_taskAB_anon_revised.tsv\")\n",
    "hs_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb98ef-a63d-492f-a48c-51bc985f0909",
   "metadata": {},
   "source": [
    "Get label distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b1b57a4-98b7-474e-9c20-93e7524f7f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HS CLASS DISTRIBUTION \n",
      "(raw) \n",
      "0    4073\n",
      "1    2766\n",
      "Name: hs, dtype: int64 \n",
      "(%) \n",
      "0    0.595555\n",
      "1    0.404445\n",
      "Name: hs, dtype: float64\n",
      "\n",
      "STEREOTYPE CLASS DISTRIBUTION \n",
      "(raw) \n",
      "0    3797\n",
      "1    3042\n",
      "Name: stereotype, dtype: int64 \n",
      "(%) \n",
      "0    0.555198\n",
      "1    0.444802\n",
      "Name: stereotype, dtype: float64\n",
      "\n",
      "CO-OCCURRENCE STATISTICS: \n",
      "hs  stereotype\n",
      "0   0             3049\n",
      "    1             1024\n",
      "1   1             2018\n",
      "    0              748\n",
      "Name: stereotype, dtype: int64 \n",
      "\n",
      "AxesSubplot(0.125,0.125;0.775x0.755)\n",
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "hs_raw = hs_dev.hs.value_counts()\n",
    "hs_norm = hs_dev.hs.value_counts(normalize=True)\n",
    "print(f'HS CLASS DISTRIBUTION \\n(raw) \\n{hs_raw} \\n(%) \\n{hs_norm}')\n",
    "ster_raw = hs_dev.stereotype.value_counts()\n",
    "ster_norm = hs_dev.stereotype.value_counts(normalize=True)\n",
    "print(f'\\nSTEREOTYPE CLASS DISTRIBUTION \\n(raw) \\n{ster_raw} \\n(%) \\n{ster_norm}')\n",
    "hs_ster = hs_dev.groupby('hs')['stereotype'].value_counts()\n",
    "print(f'\\nCO-OCCURRENCE STATISTICS: \\n{hs_ster} \\n')\n",
    "#print(sns.barplot(x=hs_norm.index, y=hs_norm))\n",
    "print(sns.barplot(x=ster_norm.index, y=ster_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b3e59e-37f6-4078-be17-88938fb0bdf1",
   "metadata": {},
   "source": [
    "Get most frequent terms and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5721869-9de4-4e66-9b90-706864ad07f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('', '', ''), 148),\n",
       " (('Studentessa', 'cinese', 'morta'), 14),\n",
       " (('#Roma', '#news', ''), 14),\n",
       " (('campo', 'rom', 'via'), 14),\n",
       " (('#news', '#Roma', ''), 14),\n",
       " (('', 'Studentessa', 'cinese'), 12),\n",
       " (('campo', 'nomadi', ''), 12),\n",
       " (('campo', 'nomadi', 'via'), 11),\n",
       " (('campo', 'rom', 'Roma'), 11),\n",
       " (('-', 'Messaggero', '#Roma'), 10),\n",
       " (('', '#ultimenotizie', '#news'), 10),\n",
       " (('#ultimenotizie', '#news', '#notizie:'), 10),\n",
       " (('cinese', 'morta', 'Roma,'), 10),\n",
       " (('campo', 'rom', ''), 10),\n",
       " (('Roma,', 'studentessa', 'cinese'), 10),\n",
       " (('', 'Mafia', 'Capitale,'), 10),\n",
       " (('', 'via', ''), 10),\n",
       " (('morta', 'Roma,', 'fermato'), 9),\n",
       " (('', 'Roma,', 'studentessa'), 9),\n",
       " (('', 'Radio', 'Londra:'), 9)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams, trigrams, FreqDist\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "STOPWORDS = get_stop_words('it') + [\"url\", \"user\", \"@user\"]\n",
    "text = [item for tweet in hs_dev['full_text'] for item in tweet.split(\" \") if item.lower() not in STOPWORDS]\n",
    "hs_text = [item for tweet in hs_dev[hs_dev.hs == 1]['full_text'] for item in tweet.split(\" \") if item.lower() not in STOPWORDS]\n",
    "ster_text = [item for tweet in hs_dev[hs_dev.stereotype == 1]['full_text'] for item in tweet.split(\" \") if item.lower() not in STOPWORDS]\n",
    "\n",
    "#find most common terms\n",
    "fdist = FreqDist(ster_text)\n",
    "fdist.most_common(20)\n",
    "\n",
    "\"\"\"\n",
    "#find most common bigrams\n",
    "bgs = bigrams(ster_text)\n",
    "fdist = FreqDist(bgs)\n",
    "fdist.most_common(20)\n",
    "\n",
    "#find most common trigrams\n",
    "tgs = trigrams(text)\n",
    "fdist = FreqDist(tgs)\n",
    "fdist.most_common(20)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b432c-69c5-4c60-85c9-06a610f04cdc",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing\n",
    "\n",
    "* tokenization\n",
    "* stopwords+punctuation removal\n",
    "* lowercasing\n",
    "* lemmatization\n",
    "* ... and many other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9380db35-ff9a-4b50-8259-8d962b7a355e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>hs</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2066</td>\n",
       "      <td>È terrorismo anche questo, per mettere in uno ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>terror mett stat soggezion person rend innocu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2045</td>\n",
       "      <td>@user @user infatti finché ci hanno guadagnato...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>infatt finc guadagn camp rom ok alemann ipocr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>Corriere: Tangenti, Mafia Capitale dimenticata...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>corr tangent maf capital dimenticatamazzett bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1259</td>\n",
       "      <td>@user ad uno ad uno, perché quando i migranti ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>quand migrant israel arriv terr canaan fuor ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>949</td>\n",
       "      <td>Il divertimento del giorno? Trovare i patrioti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>divert giorn trov patriot italian innegg rom s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>9340</td>\n",
       "      <td>Gli stati nazionali devono essere pronti a rin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stat nazional dev esser pront rinunc propr sov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>9121</td>\n",
       "      <td>Il ministro dell'interno della Germania #Horst...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ministr dell'intern german horstseehofer propo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>8549</td>\n",
       "      <td>#Salvini: In Italia troppi si sono montati la ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>salvin ital tropp mont test ringraz dio mes st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6837</th>\n",
       "      <td>9240</td>\n",
       "      <td>@user @user Chi giubila in buona fede non ha c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>giubil buon fed cap nient purtropp cred buon f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>8000</td>\n",
       "      <td>I giovani cristiani in #Etiopia sono indotti d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>giovan cristian etiop indott islam convert all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6839 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                          full_text  hs  stereotype  \\\n",
       "0     2066  È terrorismo anche questo, per mettere in uno ...   0           0   \n",
       "1     2045  @user @user infatti finché ci hanno guadagnato...   0           0   \n",
       "2       61  Corriere: Tangenti, Mafia Capitale dimenticata...   0           0   \n",
       "3     1259  @user ad uno ad uno, perché quando i migranti ...   0           0   \n",
       "4      949  Il divertimento del giorno? Trovare i patrioti...   0           0   \n",
       "...    ...                                                ...  ..         ...   \n",
       "6834  9340  Gli stati nazionali devono essere pronti a rin...   0           0   \n",
       "6835  9121  Il ministro dell'interno della Germania #Horst...   0           0   \n",
       "6836  8549  #Salvini: In Italia troppi si sono montati la ...   0           0   \n",
       "6837  9240  @user @user Chi giubila in buona fede non ha c...   0           0   \n",
       "6838  8000  I giovani cristiani in #Etiopia sono indotti d...   0           1   \n",
       "\n",
       "                                           preprocessed  \n",
       "0     terror mett stat soggezion person rend innocu ...  \n",
       "1         infatt finc guadagn camp rom ok alemann ipocr  \n",
       "2     corr tangent maf capital dimenticatamazzett bu...  \n",
       "3     quand migrant israel arriv terr canaan fuor ca...  \n",
       "4     divert giorn trov patriot italian innegg rom s...  \n",
       "...                                                 ...  \n",
       "6834  stat nazional dev esser pront rinunc propr sov...  \n",
       "6835  ministr dell'intern german horstseehofer propo...  \n",
       "6836  salvin ital tropp mont test ringraz dio mes st...  \n",
       "6837  giubil buon fed cap nient purtropp cred buon f...  \n",
       "6838  giovan cristian etiop indott islam convert all...  \n",
       "\n",
       "[6839 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, spacy, string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "def preprocess(sentence):\n",
    "    #remove digits\n",
    "    text = re.sub('\\d+', '', sentence)\n",
    "    #remove extra whitespaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    #tokenization\n",
    "    text = word_tokenize(text)\n",
    "    #stopwords/punctuation removal + lowercasing\n",
    "    text = [token.lower() for token in text if token.lower() not in STOPWORDS and token not in string.punctuation]\n",
    "    #lemmatization\n",
    "    #sent = \" \".join(text)\n",
    "    #text = [token.lemma_ for token in nlp(sent)]\n",
    "    #stemming\n",
    "    stemmer = SnowballStemmer('italian')\n",
    "    text = [stemmer.stem(token) for token in text]\n",
    "    #other possible operations: handle emojis/emoticons, hashtags, URLs/email addresses, twitter handles \n",
    "    return \" \".join(text) \n",
    "  \n",
    "\n",
    "hs_dev['preprocessed'] = hs_dev['full_text'].apply(lambda x: preprocess(x))\n",
    "hs_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1caa7-5ab1-42b9-b5f3-a8a4bac4a13a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train-Test/Validation split\n",
    "(In this case we already have a held-out test set, but you might want to use a validation set as well for hyper-parameter tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73cb891-5037-4849-aa54-c357fa18c73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       terror mett stat soggezion person rend innocu ...\n",
       "1           infatt finc guadagn camp rom ok alemann ipocr\n",
       "2       corr tangent maf capital dimenticatamazzett bu...\n",
       "3       quand migrant israel arriv terr canaan fuor ca...\n",
       "4       divert giorn trov patriot italian innegg rom s...\n",
       "                              ...                        \n",
       "6832    stat nazional dev esser pront rinunc propr sov...\n",
       "6833    ministr dell'intern german horstseehofer propo...\n",
       "6834    salvin ital tropp mont test ringraz dio mes st...\n",
       "6835    giubil buon fed cap nient purtropp cred buon f...\n",
       "6836    giovan cristian etiop indott islam convert all...\n",
       "Name: preprocessed, Length: 6837, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#if you split into train and validation sets:\n",
    "X_train_hs, X_val_hs, y_train_hs, y_val_hs = train_test_split(hs_dev[\"preprocessed\"],hs_dev[\"hs\"],test_size=0.1, shuffle=False)\n",
    "\n",
    "#if you just use the whole training set without hyper-parameter tuning:\n",
    "X_train_hs = hs_dev[\"preprocessed\"]\n",
    "y_train_hs = hs_dev[\"hs\"]\n",
    "\n",
    "X_train_hs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f621a0-d7d2-4f5b-a544-0bf49fcbe693",
   "metadata": {},
   "source": [
    "# Feature extraction and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a66791ad-6ace-4f3a-bfcd-f2496ecd37f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" alternative code using make_pipeline:\\ncount = CountVectorizer(analyzer='word', max_features=5000)\\ntfidf = TfidfVectorizer(analyzer='word', max_features=5000)\\ntfidf_trg = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=5000)\\n\\nsvm = LinearSVC()  \\nsvm_tfidf = LinearSVC()\\nsvm_trg = LinearSVC()\\n\\npipe_count = make_pipeline(count, svm)\\npipe_tfidf = make_pipeline(tfidf, svm_tfidf)\\npipe_trigrams = make_pipeline(tfidf_trg, svm_trg)\\n\\npipe_count.fit(X_train_hs, y_train_hs)\\npipe_tfidf.fit(X_train_hs, y_train_hs)\\npipe_trigrams.fit(X_train_hs, y_train_hs)\\n\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#\"\"\"\n",
    "#create feature vectors\n",
    "count = CountVectorizer(analyzer='word', max_features=5000)\n",
    "count_train = count.fit_transform(X_train_hs)\n",
    "tfidf = TfidfVectorizer(analyzer='word', max_features=5000)\n",
    "tfidf_train = tfidf.fit_transform(X_train_hs)\n",
    "trg = TfidfVectorizer(analyzer='word', ngram_range= (1,3), max_features=5000)\n",
    "trg_train = trg.fit_transform(X_train_hs)\n",
    "\n",
    "#fit the classifier on the training data\n",
    "svm = LinearSVC() # as classifier, we just use a linear SVM with default parameters\n",
    "svm_tfidf = LinearSVC()\n",
    "svm_trg = LinearSVC()\n",
    "svm.fit(count_train, y_train_hs)\n",
    "svm_tfidf.fit(tfidf_train, y_train_hs)\n",
    "svm_trg.fit(trg_train, y_train_hs)\n",
    "#\"\"\"\n",
    "\n",
    "\"\"\" alternative code using make_pipeline:\n",
    "count = CountVectorizer(analyzer='word', max_features=5000)\n",
    "tfidf = TfidfVectorizer(analyzer='word', max_features=5000)\n",
    "tfidf_trg = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=5000)\n",
    "\n",
    "svm = LinearSVC()  \n",
    "svm_tfidf = LinearSVC()\n",
    "svm_trg = LinearSVC()\n",
    "\n",
    "pipe_count = make_pipeline(count, svm)\n",
    "pipe_tfidf = make_pipeline(tfidf, svm_tfidf)\n",
    "pipe_trigrams = make_pipeline(tfidf_trg, svm_trg)\n",
    "\n",
    "pipe_count.fit(X_train_hs, y_train_hs)\n",
    "pipe_tfidf.fit(X_train_hs, y_train_hs)\n",
    "pipe_trigrams.fit(X_train_hs, y_train_hs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443801b-8296-4c1f-a3ca-7d2a73ab35a3",
   "metadata": {},
   "source": [
    "# Evaluation and error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d955b2b6-0884-4f54-b505-825e9f019da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT VECTORS: \n",
      "\n",
      "CONFUSION MATRIX \n",
      "[[275  50]\n",
      " [113  62]] \n",
      "\n",
      "EVALUATION METRICS \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.85      0.77       325\n",
      "           1       0.55      0.35      0.43       175\n",
      "\n",
      "    accuracy                           0.67       500\n",
      "   macro avg       0.63      0.60      0.60       500\n",
      "weighted avg       0.65      0.67      0.65       500\n",
      "\n",
      "TF-IDF VECTORS: \n",
      "\n",
      "CONFUSION MATRIX \n",
      "[[284  41]\n",
      " [102  73]] \n",
      "\n",
      "EVALUATION METRICS \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80       325\n",
      "           1       0.64      0.42      0.51       175\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.69      0.65      0.65       500\n",
      "weighted avg       0.70      0.71      0.70       500\n",
      "\n",
      "TF-IDF VECTORS + TRIGRAMS: \n",
      "\n",
      "CONFUSION MATRIX \n",
      "[[280  45]\n",
      " [ 98  77]] \n",
      "\n",
      "EVALUATION METRICS \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.86      0.80       325\n",
      "           1       0.63      0.44      0.52       175\n",
      "\n",
      "    accuracy                           0.71       500\n",
      "   macro avg       0.69      0.65      0.66       500\n",
      "weighted avg       0.70      0.71      0.70       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#hs_test = pd.read_csv(\"haspeede2_reference_taskAB-tweets.tsv\", sep=\"\\t\", names=['id','text','hs','stereotype'])\n",
    "hs_test = pd.read_csv(\"haspeede2_reference_taskAB-tweets_anon_revised.tsv\")\n",
    "#hs_test = pd.read_csv(\"haspeede2_reference_taskAB-news_anon_revised.tsv\")\n",
    "hs_test\n",
    "\n",
    "X_test_hs = hs_test['full_text'].apply(lambda x: preprocess(x))\n",
    "y_test_hs = hs_test['hs']\n",
    "\n",
    "#\"\"\"\n",
    "count_test = count.transform(X_test_hs)\n",
    "tfidf_test = tfidf.transform(X_test_hs)\n",
    "trg_test = trg.transform(X_test_hs)\n",
    "\n",
    "y_pred_count = svm.predict(count_test)\n",
    "y_pred_tfidf = svm_tfidf.predict(tfidf_test)\n",
    "y_pred_trg = svm_trg.predict(trg_test)\n",
    "#\"\"\"\n",
    "\"\"\"\n",
    "y_pred_count = pipe_count.predict(X_test_hs)\n",
    "y_pred_tfidf = pipe_tfidf.predict(X_test_hs)\n",
    "y_pred_trg = pipe_trigrams.predict(X_test_hs)\n",
    "\"\"\"\n",
    "\n",
    "print('COUNT VECTORS: \\n\\nCONFUSION MATRIX ')       \n",
    "print(confusion_matrix(y_test_hs, y_pred_count), '\\n') \n",
    "print('EVALUATION METRICS \\n',classification_report(y_test_hs, y_pred_count))\n",
    "print('TF-IDF VECTORS: \\n\\nCONFUSION MATRIX ')       \n",
    "print(confusion_matrix(y_test_hs, y_pred_tfidf), '\\n') \n",
    "print('EVALUATION METRICS \\n',classification_report(y_test_hs, y_pred_tfidf))\n",
    "print('TF-IDF VECTORS + TRIGRAMS: \\n\\nCONFUSION MATRIX ')  \n",
    "print(confusion_matrix(y_test_hs, y_pred_trg), '\\n') \n",
    "print('EVALUATION METRICS \\n',classification_report(y_test_hs, y_pred_trg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a67d8c-aebf-4dd8-9cd4-d7a4c24e523d",
   "metadata": {},
   "source": [
    "Try using visual libraries to explain system's predictions, like [eli5](https://github.com/TeamHG-Memex/eli5) or \n",
    "[LIME](https://marcotcr.github.io/lime/). Both packages provide nice tutorials. Here an example using eli5 (source available [here](https://github.com/TeamHG-Memex/eli5/blob/master/notebooks/Debugging%20scikit-learn%20text%20classification%20pipeline.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02207713-d85c-4b97-8774-2ca74a09c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = \"Gli islamici sono tutti terroristi da mandare al rogo\"\n",
    "tfidf_test = tfidf.transform(X_test_hs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
