{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8881a96-801e-4e54-b68f-9706ccdfd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim matplotlib nltk numpy pandas scikit-learn seaborn spacy stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27447cda-20a4-4a63-a0bb-c48e1beb0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download it_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdb6476-34af-437e-9ba4-cae2a67fda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b84796bf-445f-4d38-88a7-8fa3f37fb8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"it_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2303199-6329-420c-bc92-d120cc67d8e7",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "19219b72-c712-4823-9086-47ccc38574b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hs</th>\n",
       "      <th>stereotype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2066</td>\n",
       "      <td>È terrorismo anche questo, per mettere in uno ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2045</td>\n",
       "      <td>@user @user infatti finché ci hanno guadagnato...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>Corriere: Tangenti, Mafia Capitale dimenticata...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1259</td>\n",
       "      <td>@user ad uno ad uno, perché quando i migranti ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>949</td>\n",
       "      <td>Il divertimento del giorno? Trovare i patrioti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6832</th>\n",
       "      <td>9340</td>\n",
       "      <td>Gli stati nazionali devono essere pronti a rin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6833</th>\n",
       "      <td>9121</td>\n",
       "      <td>Il ministro dell'interno della Germania #Horst...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>8549</td>\n",
       "      <td>#Salvini: In Italia troppi si sono montati la ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>9240</td>\n",
       "      <td>@user @user Chi giubila in buona fede non ha c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>8000</td>\n",
       "      <td>I giovani cristiani in #Etiopia sono indotti d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6837 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              text   hs  stereotype\n",
       "0     2066  È terrorismo anche questo, per mettere in uno ...   0           0\n",
       "1     2045  @user @user infatti finché ci hanno guadagnato...   0           0\n",
       "2       61  Corriere: Tangenti, Mafia Capitale dimenticata...   0           0\n",
       "3     1259  @user ad uno ad uno, perché quando i migranti ...   0           0\n",
       "4      949  Il divertimento del giorno? Trovare i patrioti...   0           0\n",
       "...    ...                                                ...  ..         ...\n",
       "6832  9340  Gli stati nazionali devono essere pronti a rin...   0           0\n",
       "6833  9121  Il ministro dell'interno della Germania #Horst...   0           0\n",
       "6834  8549  #Salvini: In Italia troppi si sono montati la ...   0           0\n",
       "6835  9240  @user @user Chi giubila in buona fede non ha c...   0           0\n",
       "6836  8000  I giovani cristiani in #Etiopia sono indotti d...   0           1\n",
       "\n",
       "[6837 rows x 4 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hs_dev = pd.read_csv(\"haspeede2_dev_taskAB.tsv\", sep=\"\\t\")\n",
    "hs_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb98ef-a63d-492f-a48c-51bc985f0909",
   "metadata": {},
   "source": [
    "Get label distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1b57a4-98b7-474e-9c20-93e7524f7f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HS CLASS DISTRIBUTION \n",
      "(raw) \n",
      "0    4071\n",
      "1    2766\n",
      "Name: hs, dtype: int64 \n",
      "(%) \n",
      "0    0.595437\n",
      "1    0.404563\n",
      "Name: hs, dtype: float64\n",
      "\n",
      "STEREOTYPE CLASS DISTRIBUTION \n",
      "(raw) \n",
      "0    3796\n",
      "1    3041\n",
      "Name: stereotype, dtype: int64 \n",
      "(%) \n",
      "0    0.555214\n",
      "1    0.444786\n",
      "Name: stereotype, dtype: float64\n",
      "\n",
      "CO-OCCURRENCE STATISTICS: \n",
      "hs  stereotype\n",
      "0   0             3048\n",
      "    1             1023\n",
      "1   1             2018\n",
      "    0              748\n",
      "Name: stereotype, dtype: int64 \n",
      "\n",
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "hs_raw = hs_dev.hs.value_counts()\n",
    "hs_norm = hs_dev.hs.value_counts(normalize=True)\n",
    "print(f'HS CLASS DISTRIBUTION \\n(raw) \\n{hs_raw} \\n(%) \\n{hs_norm}')\n",
    "ster_raw = hs_dev.stereotype.value_counts()\n",
    "ster_norm = hs_dev.stereotype.value_counts(normalize=True)\n",
    "print(f'\\nSTEREOTYPE CLASS DISTRIBUTION \\n(raw) \\n{ster_raw} \\n(%) \\n{ster_norm}')\n",
    "hs_ster = hs_dev.groupby('hs')['stereotype'].value_counts()\n",
    "print(f'\\nCO-OCCURRENCE STATISTICS: \\n{hs_ster} \\n')\n",
    "#print(sns.barplot(x=hs_norm.index, y=hs_norm))\n",
    "print(sns.barplot(x=ster_norm.index, y=ster_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b3e59e-37f6-4078-be17-88938fb0bdf1",
   "metadata": {},
   "source": [
    "Get most frequent terms and n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5721869-9de4-4e66-9b90-706864ad07f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('', '', ''), 149),\n",
       " (('Studentessa', 'cinese', 'morta'), 14),\n",
       " (('#Roma', '#news', ''), 14),\n",
       " (('campo', 'rom', 'via'), 14),\n",
       " (('#news', '#Roma', ''), 14),\n",
       " (('', 'Studentessa', 'cinese'), 12),\n",
       " (('campo', 'nomadi', ''), 12),\n",
       " (('campo', 'nomadi', 'via'), 11),\n",
       " (('campo', 'rom', 'Roma'), 11),\n",
       " (('-', 'Messaggero', '#Roma'), 10),\n",
       " (('', '#ultimenotizie', '#news'), 10),\n",
       " (('#ultimenotizie', '#news', '#notizie:'), 10),\n",
       " (('cinese', 'morta', 'Roma,'), 10),\n",
       " (('campo', 'rom', ''), 10),\n",
       " (('Roma,', 'studentessa', 'cinese'), 10),\n",
       " (('', 'Mafia', 'Capitale,'), 10),\n",
       " (('', 'via', ''), 10),\n",
       " (('morta', 'Roma,', 'fermato'), 9),\n",
       " (('', 'Roma,', 'studentessa'), 9),\n",
       " (('', 'Radio', 'Londra:'), 9)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams, trigrams, FreqDist\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "STOPWORDS = get_stop_words('it') + [\"url\", \"user\", \"@user\"]\n",
    "text = [item for tweet in hs_dev['text '] for item in tweet.split(\" \") if item.lower() not in STOPWORDS]\n",
    "hs_text = [item for tweet in hs_dev[hs_dev.hs == 1]['text '] for item in tweet.split(\" \") if item.lower() not in STOPWORDS]\n",
    "ster_text = [item for tweet in hs_dev[hs_dev.stereotype == 1]['text '] for item in tweet.split(\" \") if item.lower() not in STOPWORDS]\n",
    "\n",
    "#find most common terms\n",
    "fdist = FreqDist(ster_text)\n",
    "fdist.most_common(20)\n",
    "\n",
    "#find most common bigrams\n",
    "bgs = bigrams(ster_text)\n",
    "fdist = FreqDist(bgs)\n",
    "fdist.most_common(20)\n",
    "\n",
    "#find most common trigrams\n",
    "tgs = trigrams(text)\n",
    "fdist = FreqDist(tgs)\n",
    "fdist.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b432c-69c5-4c60-85c9-06a610f04cdc",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing\n",
    "\n",
    "* tokenization\n",
    "* stopwords+punctuation removal\n",
    "* lowercasing\n",
    "* lemmatization\n",
    "* ... and many other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9380db35-ff9a-4b50-8259-8d962b7a355e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hs</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2066</td>\n",
       "      <td>È terrorismo anche questo, per mettere in uno ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>terror mett stat soggezion person rend innocu ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2045</td>\n",
       "      <td>@user @user infatti finché ci hanno guadagnato...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>infatt finc guadagn camp rom ok alemann ipocr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>Corriere: Tangenti, Mafia Capitale dimenticata...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>corr tangent maf capital dimenticatamazzett bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1259</td>\n",
       "      <td>@user ad uno ad uno, perché quando i migranti ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>quand migrant israel arriv terr canaan fuor ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>949</td>\n",
       "      <td>Il divertimento del giorno? Trovare i patrioti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>divert giorn trov patriot italian innegg rom s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6832</th>\n",
       "      <td>9340</td>\n",
       "      <td>Gli stati nazionali devono essere pronti a rin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>stat nazional dev esser pront rinunc propr sov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6833</th>\n",
       "      <td>9121</td>\n",
       "      <td>Il ministro dell'interno della Germania #Horst...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ministr dell'intern german horstseehofer propo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6834</th>\n",
       "      <td>8549</td>\n",
       "      <td>#Salvini: In Italia troppi si sono montati la ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>salvin ital tropp mont test ringraz dio mes st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6835</th>\n",
       "      <td>9240</td>\n",
       "      <td>@user @user Chi giubila in buona fede non ha c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>giubil buon fed cap nient purtropp cred buon f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6836</th>\n",
       "      <td>8000</td>\n",
       "      <td>I giovani cristiani in #Etiopia sono indotti d...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>giovan cristian etiop indott islam convert all...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6837 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              text   hs  stereotype  \\\n",
       "0     2066  È terrorismo anche questo, per mettere in uno ...   0           0   \n",
       "1     2045  @user @user infatti finché ci hanno guadagnato...   0           0   \n",
       "2       61  Corriere: Tangenti, Mafia Capitale dimenticata...   0           0   \n",
       "3     1259  @user ad uno ad uno, perché quando i migranti ...   0           0   \n",
       "4      949  Il divertimento del giorno? Trovare i patrioti...   0           0   \n",
       "...    ...                                                ...  ..         ...   \n",
       "6832  9340  Gli stati nazionali devono essere pronti a rin...   0           0   \n",
       "6833  9121  Il ministro dell'interno della Germania #Horst...   0           0   \n",
       "6834  8549  #Salvini: In Italia troppi si sono montati la ...   0           0   \n",
       "6835  9240  @user @user Chi giubila in buona fede non ha c...   0           0   \n",
       "6836  8000  I giovani cristiani in #Etiopia sono indotti d...   0           1   \n",
       "\n",
       "                                           preprocessed  \n",
       "0     terror mett stat soggezion person rend innocu ...  \n",
       "1         infatt finc guadagn camp rom ok alemann ipocr  \n",
       "2     corr tangent maf capital dimenticatamazzett bu...  \n",
       "3     quand migrant israel arriv terr canaan fuor ca...  \n",
       "4     divert giorn trov patriot italian innegg rom s...  \n",
       "...                                                 ...  \n",
       "6832  stat nazional dev esser pront rinunc propr sov...  \n",
       "6833  ministr dell'intern german horstseehofer propo...  \n",
       "6834  salvin ital tropp mont test ringraz dio mes st...  \n",
       "6835  giubil buon fed cap nient purtropp cred buon f...  \n",
       "6836  giovan cristian etiop indott islam convert all...  \n",
       "\n",
       "[6837 rows x 5 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, spacy, string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "def preprocess(sentence):\n",
    "    #remove digits\n",
    "    text = re.sub('\\d+', '', sentence)\n",
    "    #remove extra whitespaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    #tokenization\n",
    "    text = word_tokenize(text)\n",
    "    #stopwords/punctuation removal + lowercasing\n",
    "    text = [token.lower() for token in text if token.lower() not in STOPWORDS and token not in string.punctuation]\n",
    "    #lemmatization\n",
    "    #sent = \" \".join(text)\n",
    "    #text = [token.lemma_ for token in nlp(sent)]\n",
    "    #stemming\n",
    "    stemmer = SnowballStemmer('italian')\n",
    "    text = [stemmer.stem(token) for token in text]\n",
    "    #other possible operations: handle emojis/emoticons, hashtags, URLs/email addresses, twitter handles \n",
    "    return \" \".join(text) \n",
    "  \n",
    "\n",
    "hs_dev['preprocessed'] = hs_dev['text '].apply(lambda x: preprocess(x))\n",
    "hs_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1caa7-5ab1-42b9-b5f3-a8a4bac4a13a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train-Test/Validation split\n",
    "(In this case we already have a held-out test set, but you might want to use a validation set as well for hyper-parameter tuning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d73cb891-5037-4849-aa54-c357fa18c73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       terror mett stat soggezion person rend innocu ...\n",
       "1           infatt finc guadagn camp rom ok alemann ipocr\n",
       "2       corr tangent maf capital dimenticatamazzett bu...\n",
       "3       quand migrant israel arriv terr canaan fuor ca...\n",
       "4       divert giorn trov patriot italian innegg rom s...\n",
       "                              ...                        \n",
       "6832    stat nazional dev esser pront rinunc propr sov...\n",
       "6833    ministr dell'intern german horstseehofer propo...\n",
       "6834    salvin ital tropp mont test ringraz dio mes st...\n",
       "6835    giubil buon fed cap nient purtropp cred buon f...\n",
       "6836    giovan cristian etiop indott islam convert all...\n",
       "Name: preprocessed, Length: 6837, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#if you split into train and validation sets:\n",
    "X_train_hs, X_val_hs, y_train_hs, y_val_hs = train_test_split(hs_dev[\"preprocessed\"],hs_dev[\"hs\"],test_size=0.1, shuffle=False)\n",
    "\n",
    "#if you just use the whole training set without hyper-parameter tuning:\n",
    "X_train_hs = hs_dev[\"preprocessed\"]\n",
    "y_train_hs = hs_dev[\"hs\"]\n",
    "\n",
    "X_train_hs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f621a0-d7d2-4f5b-a544-0bf49fcbe693",
   "metadata": {},
   "source": [
    "# Feature extraction and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a66791ad-6ace-4f3a-bfcd-f2496ecd37f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer(max_features=5000)),\n",
       "                ('linearsvc', LinearSVC())])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\"\"\"\n",
    "#create feature vectors\n",
    "count = CountVectorizer(analyzer='word', max_features=5000)\n",
    "count_train = count.fit_transform(X_train_hs)\n",
    "tfidf = TfidfVectorizer(analyzer='word', max_features=5000)\n",
    "tfidf_train = tfidf.fit_transform(X_train_hs)\n",
    "\n",
    "#fit the classifier on the training data\n",
    "svm = LinearSVC() \n",
    "svm_tfidf = LinearSVC()\n",
    "svm.fit(count_train, y_train_hs)\n",
    "svm_tfidf.fit(tfidf_train, y_train_hs)\n",
    "\"\"\"\n",
    "count = CountVectorizer(analyzer='word', max_features=5000)\n",
    "tfidf = TfidfVectorizer(analyzer='word', max_features=5000)\n",
    "#tfidf_tri = TfidfVectorizer(analyzer='word', ngram_range=(1,3), max_features=5000)\n",
    "\n",
    "svm = LinearSVC()  # as classifier, we just use a linear SVM with default parameters\n",
    "\n",
    "pipe_count = make_pipeline(count, svm)\n",
    "pipe_tfidf = make_pipeline(tfidf, svm)\n",
    "#pipe_trigrams = make_pipeline(tfidf_tri, svm)\n",
    "\n",
    "pipe_count.fit(X_train_hs, y_train_hs)\n",
    "pipe_tfidf.fit(X_train_hs, y_train_hs)\n",
    "#pipe_trigrams.fit(X_train_hs, y_train_hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443801b-8296-4c1f-a3ca-7d2a73ab35a3",
   "metadata": {},
   "source": [
    "# Evaluation and error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d955b2b6-0884-4f54-b505-825e9f019da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.43      0.57       641\n",
      "           1       0.61      0.90      0.72       622\n",
      "\n",
      "    accuracy                           0.66      1263\n",
      "   macro avg       0.71      0.67      0.65      1263\n",
      "weighted avg       0.71      0.66      0.64      1263\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.75      0.75       641\n",
      "           1       0.74      0.75      0.75       622\n",
      "\n",
      "    accuracy                           0.75      1263\n",
      "   macro avg       0.75      0.75      0.75      1263\n",
      "weighted avg       0.75      0.75      0.75      1263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "hs_test = pd.read_csv(\"haspeede2_reference_taskAB-tweets.tsv\", sep=\"\\t\", names=['id','text','hs','stereotype'])\n",
    "hs_test\n",
    "\n",
    "X_test_hs = hs_test['text'].apply(lambda x: preprocess(x))\n",
    "y_test_hs = hs_test['hs']\n",
    "\n",
    "y_pred_count = pipe_count.predict(X_test_hs)\n",
    "print(classification_report(y_test_hs, y_pred_count))\n",
    "\n",
    "y_pred_tfidf = pipe_tfidf.predict(X_test_hs)\n",
    "print(classification_report(y_test_hs, y_pred_tfidf))\n",
    "\n",
    "#y_pred_trigrams = pipe_trigrams.predict(X_test_hs)\n",
    "#print(classification_report(y_test_hs, y_pred_trigrams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d1c74-f89f-413e-9a0f-1dfaa7dab200",
   "metadata": {},
   "source": [
    "Create a confusion matrix to see the classifier's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "aba2b260-0749-4702-9d72-72fc2bd5b0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX \n",
      " - with count vectors:\n",
      "[[278 363]\n",
      " [ 62 560]] \n",
      "\n",
      " - with tfidf vectors:\n",
      "[[481 160]\n",
      " [156 466]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('CONFUSION MATRIX \\n - with count vectors:')       \n",
    "print(confusion_matrix(y_test_hs, y_pred_count), '\\n') \n",
    "\n",
    "print(' - with tfidf vectors:')       \n",
    "print(confusion_matrix(y_test_hs, y_pred_tfidf), '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da369f76-5088-45a7-ae92-beaea662f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lime eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a67d8c-aebf-4dd8-9cd4-d7a4c24e523d",
   "metadata": {},
   "source": [
    "Try using visual libraries to explain system's predictions, like [eli5](https://github.com/TeamHG-Memex/eli5) or \n",
    "[LIME](https://marcotcr.github.io/lime/). Both packages provide nice tutorials. Here an example using eli5 (source available [here](https://github.com/TeamHG-Memex/eli5/blob/master/notebooks/Debugging%20scikit-learn%20text%20classification%20pipeline.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a8d4c19e-0417-436b-af16-d1092ea38191",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearSVC' object has no attribute 'support_vectors_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [157]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\n\u001b[0;32m      3\u001b[0m svm\u001b[38;5;241m.\u001b[39mcoef_\n\u001b[1;32m----> 4\u001b[0m \u001b[43msvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_vectors_\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LinearSVC' object has no attribute 'support_vectors_'"
     ]
    }
   ],
   "source": [
    "import eli5\n",
    "\n",
    "svm.coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7509c1eb-c888-4e78-9174-cc90bd1f1559",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [158]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43meli5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43msvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_hs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jun2022\\lib\\site-packages\\eli5\\ipython.py:307\u001b[0m, in \u001b[0;36mshow_prediction\u001b[1;34m(estimator, doc, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\" Return an explanation of estimator prediction\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03mas an IPython.display.HTML object. Use this function\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03mto show information about classifier prediction in IPython.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m \n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    306\u001b[0m format_kwargs, explain_kwargs \u001b[38;5;241m=\u001b[39m _split_kwargs(kwargs)\n\u001b[1;32m--> 307\u001b[0m expl \u001b[38;5;241m=\u001b[39m explain_prediction(estimator, doc, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexplain_kwargs)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expl\u001b[38;5;241m.\u001b[39mimage \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;66;03m# dispatch to image display implementation\u001b[39;00m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(format_as_image, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jun2022\\lib\\functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    887\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dispatch(args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jun2022\\lib\\site-packages\\eli5\\sklearn\\explain_prediction.py:170\u001b[0m, in \u001b[0;36mexplain_prediction_linear_classifier\u001b[1;34m(clf, doc, vec, top, top_targets, target_names, targets, feature_names, feature_re, feature_filter, vectorized)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03mExplain prediction of a linear classifier.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03mis already vectorized.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    169\u001b[0m vec, feature_names \u001b[38;5;241m=\u001b[39m handle_vec(clf, doc, vec, vectorized, feature_names)\n\u001b[1;32m--> 170\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mget_X\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_dense\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m proba \u001b[38;5;241m=\u001b[39m predict_proba(clf, X)\n\u001b[0;32m    173\u001b[0m score, \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mdecision_function(X)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jun2022\\lib\\site-packages\\eli5\\sklearn\\utils.py:233\u001b[0m, in \u001b[0;36mget_X\u001b[1;34m(doc, vec, vectorized, to_dense)\u001b[0m\n\u001b[0;32m    231\u001b[0m         X \u001b[38;5;241m=\u001b[39m doc\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 233\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m to_dense \u001b[38;5;129;01mand\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m    235\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mtoarray()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jun2022\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1379\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_vocabulary()\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;66;03m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m _, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1381\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jun2022\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jun2022\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\jun2022\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "eli5.show_prediction(svm, y_test_hs[0], vec=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02207713-d85c-4b97-8774-2ca74a09c5a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
